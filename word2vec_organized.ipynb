{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delphi import Delphi as DB\n",
    "from creds import my_creds\n",
    "db = DB()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from gensim.test.utils import common_texts\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_10_data = pd.read_fwf('icd10cm_codes_2020.txt',header=None, names=[\"icd_10_code\", \"icd_10_title\"])\n",
    "print(icd_10_data)\n",
    "icd_10_data['icd_10_code_first_digits']=icd_10_data['icd_10_code'].str[:3]\n",
    "print(icd_10_data)\n",
    "icd_10_data['icd_10_code_first_digits']=icd_10_data['icd_10_code'].str[:3]\n",
    "merged_inner = icd_10_data\n",
    "print(merged_inner)\n",
    "doctor_notes_exp=merged_inner[['icd_10_title','icd_10_code_first_digits']]\n",
    "print(doctor_notes_exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hier_text_preprocessing(df):\n",
    "    texts=[]\n",
    "    all_sentences = []\n",
    "    df_grouped = df.groupby(['icd_10_code_first_digits']).agg({'icd_10_title': lambda x: ' '.join(x)})\n",
    "    #print(df_grouped)\n",
    "    for index in range(len(df_grouped)):\n",
    "        texts.append(df_grouped.icd_10_title[index])\n",
    "    print(texts[:5])\n",
    "    sentences = []\n",
    "    for index in range(len(texts)):\n",
    "        sentences = [re.sub(pattern=r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]', \n",
    "                            repl='', \n",
    "                            string=x\n",
    "                           ).strip().split(' ') for x in texts[index].split('\\n') \n",
    "                     if not x.endswith('writes:')]\n",
    "        \"\"\" sentences = [re.sub(pattern=r'[0-9]+', \n",
    "                            repl='', \n",
    "                            string=x\n",
    "                           ).strip().split(' ') for x in texts[index].split('\\n') \n",
    "                     if not x.endswith('writes:')]\"\"\"\n",
    "        #sentences = ''.join([i for i in sentences if not i.isdigit()])\n",
    "        sentences = [x for x in sentences if x != ['']]\n",
    "        sentences=[[string.lower() for string in sublist] for sublist in sentences]\n",
    "        sentences=[[re.sub('[^A-Za-z0-9]+', '', string) for string in sublist] for sublist in sentences]\n",
    "        texts[index] = sentences\n",
    "    for text in texts:\n",
    "        all_sentences += text\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences=hier_text_preprocessing(doctor_notes_exp) \n",
    "#so all_sentences is a nested list consisting of words,\n",
    "#in which case I'll need to decide which hierarchies to preserve, or i could try out the performance and see\n",
    "#the problem is that, some of them don't have low level codes. maybe try use level 1 and level 2 code titles\n",
    "#for experiments\n",
    "print(all_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'unspecified', 'due', 'involvement', 'manifestation', 'west', 'nile', 'identified']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'phrases = Phrases(all_sentences, common_terms=common_terms)\\n# The Phraser object is used from now on to transform sentences\\nbigram = Phraser(phrases)\\n\\n# Applying the Phraser to transform our sentences is simply\\nall_sentences = list(bigram[all_sentences])\\nall_sentences\\nprint(common_terms)\\nphrases = Phrases(all_sentences, common_terms=common_terms)\\n# The Phraser object is used from now on to transform sentences\\nbigram = Phraser(phrases)\\n\\n# Applying the Phraser to transform our sentences is simply\\nall_sentences = list(bigram[all_sentences])\\nall_sentences'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_terms =stopwords.words('english')\n",
    "common_terms=common_terms+['unspecified','due','involvement','manifestation','west','nile','identified']\n",
    "print(common_terms)\n",
    "phrases = Phrases(all_sentences, common_terms=common_terms)\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "# Applying the Phraser to transform our sentences is simply\n",
    "all_sentences = list(bigram[all_sentences])\n",
    "all_sentences\n",
    "print(common_terms)\n",
    "phrases = Phrases(all_sentences, common_terms=common_terms)\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "# Applying the Phraser to transform our sentences is simply\n",
    "all_sentences = list(bigram[all_sentences])\n",
    "all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data=[['cholera','cholera','vibrio','cholerae','biovar', 'elto', 'cholera','vibrio','cholerae','biovar', 'cholerae'],['infectious','gastroenteritis','colitis']]\n",
    "#hier_icd_10_titles=hier_text_preprocessing(icd_10_data)\n",
    "#common_terms =stopwords.words('english')\n",
    "#common_terms=common_terms+['unspecified','due','involvement','manifestation','west','nile','identified']\n",
    "#phrases = Phrases(hier_icd_10_titles, common_terms=common_terms)\n",
    "# The Phraser object is used from now on to transform sentences\n",
    "#bigram = Phraser(phrases)\n",
    "\n",
    "# Applying the Phraser to transform our sentences is simply\n",
    "#all_sentences = list(bigram[hier_icd_10_titles])\n",
    "#print(all_sentences[:5])\n",
    "model = Word2Vec(all_sentences, \n",
    "                 min_count=3,   # Ignore words that appear less than this\n",
    "                 size=50,      # Dimensionality of word embeddings\n",
    "                 workers=4,     # Number of processors (parallelisation)\n",
    "                 sg=1,\n",
    "                 hs=1,\n",
    "                 negative=5,\n",
    "                 ns_exponent=0.75,\n",
    "                 cbow_mean=0.5,\n",
    "                 window=5,      # Context window for words during training\n",
    "                 iter=30,\n",
    "                 compute_loss=True)       # Number of epochs training over corpus\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.wv.vectors.shape)\n",
    "\"\"\"model.vector_size\n",
    "len(model.wv.vocab)\"\"\"\n",
    "#model.most_similar('pain unspecified')\n",
    "#print(model.wv.most_similar('vibrio'))\n",
    "#print(model['vibrio'])\n",
    "#model.wv.most_similar('unspecified')\n",
    "\"\"\"print(model.wv.most_similar('salmonella'))\n",
    "print(model.wv.most_similar('rabies'))\"\"\"\n",
    "model.get_latest_training_loss()\n",
    "model.save(\"notes_word2vec_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
